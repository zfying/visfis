{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762eaa05-190a-4827-a92d-08ef98c290cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# get hints for GQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb1020-d8fe-476e-9ee5-bea3aa0fdcef",
   "metadata": {},
   "source": [
    "### get train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d6857-ad1d-43d1-9d24-8155051cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d528359-8000-45e1-bc89-14284600e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/questions/train_questions.json'\n",
    "train_qns = json.load(open(_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8268b-77d8-49cb-bf7f-b6f48b1bdd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_qns['questions']), train_qns['questions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1b522-3ead-44a1-8b34-fd84d41e1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_qns['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d229795-000d-4757-bd99-5681d9440123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qns['questions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ce3de-d5f9-4bba-ba93-fa6363861940",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qns_subset = {}\n",
    "train_qns_subset['questions'] = train_qns['questions'][:int(len(train_qns['questions'])/6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6ae82-204a-4c16-ae3c-47a978270064",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_qns_subset['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e2117-e808-46fa-bd00-f71e401649a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/questions/train_annotations.json'\n",
    "train_anns = json.load(open(_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0c493-7b33-45a9-a37b-54262d4f1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_qid = set()\n",
    "for qn in train_qns_subset['questions']:\n",
    "    subset_qid.add(qn['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e93b40-ef51-4430-985f-11b1077711d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subset_qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0a985-c8b2-406c-b7a0-854b920fa4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anns_subset = {}\n",
    "train_anns_subset['annotations'] = []\n",
    "for ann in train_anns['annotations']:\n",
    "    if ann['question_id'] in subset_qid:\n",
    "        train_anns_subset['annotations'].append(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83891411-39de-46ce-b904-5d695a2ec2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_anns_subset['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8cb9d-db80-47c5-bba4-30f4cfca77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/questions/train-100k_questions.json'\n",
    "with open(_path, 'w') as f:\n",
    "    json.dump(train_qns_subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d7aec-d576-4efc-b3bc-76a429e7fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/questions/train-100k_annotations.json'\n",
    "with open(_path, 'w') as f:\n",
    "    json.dump(train_anns_subset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569055a-c312-4c2a-b962-e2e61639685f",
   "metadata": {},
   "source": [
    "### convert hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb9f59-1121-4b7c-8d41-658105c60364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "_path = '../data/neg_gqacp/hints/gqacp_hints_random.pkl'\n",
    "with open(_path, 'rb') as f:\n",
    "    hints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d296d8-5d5b-4243-87f3-9b562a82e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid in hints:\n",
    "    print(hints[qid])\n",
    "    break\n",
    "    # hints[qid] = hints[qid].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baa29a-8b45-466b-ae25-89e85efc4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_path, 'wb') as handle:\n",
    "    pickle.dump(hints, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e1360-6726-4f68-a1c4-f8f2b7bc1c9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## generate importance map using scene graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12beff7-682d-429d-9b0f-9beae820fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01ff1e-913e-43a3-aecd-48bc359a528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impt_objs_id_from_qns(data_root, split):\n",
    "    print(f\"Reading {split} question file...\")\n",
    "    gqa_questions = json.load(open(os.path.join(data_root, 'questions', f'{split}_balanced_questions.json')))\n",
    "\n",
    "    print(f\"Finding impt objs...\")\n",
    "    qid2impt_objs_ids = {}\n",
    "    qid2imgid = {}\n",
    "    for qid, qns in tqdm(gqa_questions.items()):\n",
    "        obj_ids = []\n",
    "        for s in qns['semantic']:\n",
    "            obj_id = re.findall('[0-9]+', s['argument'])\n",
    "            obj_ids += obj_id\n",
    "        qid2impt_objs_ids[qid] = obj_ids\n",
    "        qid2imgid[qid] = qns['imageId']\n",
    "    return qid2impt_objs_ids, qid2imgid\n",
    "\n",
    "def get_impt_map(data_root, split):\n",
    "    qid2impt_objs_ids, qid2imgid = get_impt_objs_id_from_qns(data_root, split)\n",
    "    \n",
    "    print(f\"Reading {split} scene graph file...\")\n",
    "    gqa_scenegraph = json.load(open(os.path.join(data_root, 'sceneGraph', f'{split}_sceneGraphs.json')))\n",
    "    \n",
    "    print(f\"Generating {split} masks...\")\n",
    "    for qid, ids_list in tqdm(qid2impt_objs_ids.items()):\n",
    "\n",
    "        img_id = qid2imgid[qid]\n",
    "        cur_scene = gqa_scenegraph[img_id]\n",
    "        w = cur_scene['width']\n",
    "        h = cur_scene['height']\n",
    "        img = np.zeros((h, w))\n",
    "\n",
    "        for obj_id in ids_list:\n",
    "            if len(obj_id) <= 3: # accidentally includes non obj ids\n",
    "                continue \n",
    "            # get obj info\n",
    "            obj = cur_scene['objects'][obj_id]\n",
    "            obj_h = obj['h']\n",
    "            obj_w = obj['w']\n",
    "            obj_x = obj['x']\n",
    "            obj_y = obj['y']\n",
    "\n",
    "            img[obj_y:obj_y+obj_h, obj_x:obj_x+obj_w] = np.ones(img[obj_y:obj_y+obj_h, obj_x:obj_x+obj_w].shape)\n",
    "\n",
    "        # save img\n",
    "        img = Image.fromarray(img*255)\n",
    "        img = img.convert('RGB')\n",
    "        img.save(os.path.join(data_root, \"masks\", split,\"GQA_\"+qid+\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5b270-44ef-46e0-996c-284d9b55ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data/neg_gqa/GQA/\"\n",
    "get_impt_map(data_root, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf5774-91dd-49de-abc2-51a37e432566",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## sanity check the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeca512-6ed3-4b68-8c76-efc320d03f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "gqa_data_path = '../data/neg_gqa/GQA/'\n",
    "split = 'train'\n",
    "gqa_questions = json.load(open(os.path.join(gqa_data_path,f'./questions/{split}_balanced_questions.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35becf9-dabf-4a94-9bd6-18b48103b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "qid = random.choice(list(gqa_questions))\n",
    "print(gqa_questions[qid]['question'])\n",
    "img_id = gqa_questions[qid]['imageId']\n",
    "img_ori = Image.open(os.path.join(gqa_data_path,f'images/images/{img_id}.jpg'))\n",
    "img_mask = Image.open(os.path.join(gqa_data_path,f'masks/{split}/GQA_{qid}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d5b7a-6cbe-4a92-a614-c6e2a3b0e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = Image.new('RGB', (img_ori.width + img_mask.width, img_mask.height))\n",
    "dst.paste(img_ori, (0, 0))\n",
    "dst.paste(img_mask, (img_ori.width, 0))\n",
    "\n",
    "plt.imshow(dst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7922499-0c9f-4408-a952-940e75163791",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## get hints from masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6ae49-16c2-4777-bb9f-0bafda786cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e67cb-b045-4859-9e15-966077475677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(img_id, bbox_scores, spatials):\n",
    "    # original image\n",
    "    img = cv2.imread(os.path.join(gqa_data_path,f'images/images/{img_id}.jpg'))\n",
    "    h, w, _ = img.shape\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    # bbox image\n",
    "    h, w, _ = img.shape\n",
    "    bbox_img = img.copy()\n",
    "    for obj in spatials:\n",
    "        x1, y1, x2, y2, _, _ = obj\n",
    "        cv2.rectangle(bbox_img, \n",
    "                      (int(x1*w), int(y1*h)), \n",
    "                      (int(x2*w), int(y2*h)), \n",
    "                      (255,0,0), 2)\n",
    "    plt.imshow(bbox_img)\n",
    "    plt.show() \n",
    "    \n",
    "    mask = torch.zeros(img.shape[0], img.shape[1])\n",
    "    # get the max score for diff bbox\n",
    "    for index in range(len(bbox_scores)):\n",
    "        x1, y1, x2, y2, _, _ = spatials[index]\n",
    "        curr_score_tensor = mask[int(y1*h):int(y2*h), int(x1*w):int(x2*w)] # DEBUG!\n",
    "        new_score_tensor = torch.ones_like(curr_score_tensor)*bbox_scores[index].item()\n",
    "        mask[int(y1*h):int(y2*h), int(x1*w):int(x2*w)] = torch.max(new_score_tensor,\\\n",
    "                                                                   mask[int(y1*h):int(y2*h), int(x1*w):int(x2*w)])\n",
    "    mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "    mask_norm = mask.cpu().data.numpy()\n",
    "    # get masked img\n",
    "    mask = mask.unsqueeze_(-1)\n",
    "    mask = mask.expand(img.shape)\n",
    "    masked_img = img * mask.cpu().data.numpy()\n",
    "\n",
    "    print(mask_norm.shape, mask.shape)\n",
    "    plt.imshow(mask)\n",
    "    plt.show()\n",
    "    return masked_img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e93b1-eadf-4808-9f1d-376239878e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help func: calculate importance score\n",
    "def calc_att_score(bbox, att_map, SOFT=False):\n",
    "    if SOFT:\n",
    "        mask = att_map == 0\n",
    "        att_map = att_map + np.ones(att_map.shape)*0.1 * mask\n",
    "    # bbox: x1, y1, x2, y2 (scaled localtion)\n",
    "    # att_map: \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    region_area = np.abs(x1 - x2) * np.abs(y1 - y2)\n",
    "    assert(len(att_map.shape) == 2)\n",
    "    h = att_map.shape[0]\n",
    "    w = att_map.shape[1]\n",
    "    \n",
    "    score_inside = np.sum(att_map[int(y1*h):int(y2*h), int(x1*w):int(x2*w)]) # DEBUG!\n",
    "    score_outside = np.sum(att_map) - score_inside\n",
    "    score_inside = score_inside / region_area\n",
    "    score_outside = score_outside / (1.0 - region_area)\n",
    "    importance = score_inside / (score_inside + score_outside)\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c909a5-8838-4e86-8bbd-efa805f389cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hint_scores_from_masks(split):\n",
    "    # read questions\n",
    "    gqa_questions = json.load(open(f\"../data/neg_gqa/questions/{split}_questions.json\"))['questions']\n",
    "    # read spatials\n",
    "    h5_path = f\"../data/neg_gqa/{split}36.hdf5\"\n",
    "    hf = h5py.File(h5_path, 'r')\n",
    "    spatials = hf.get('spatial_features')\n",
    "    # read img_id2idx\n",
    "    image_id2ix = pickle.load(open(f\"../data/neg_gqa/{split}36_imgid2img.pkl\", 'rb'))\n",
    "    \n",
    "    qid2hints = {}\n",
    "    VISUALIZE = False\n",
    "    for qn in tqdm(gqa_questions):\n",
    "        # read\n",
    "        img_id = qn['image_id']\n",
    "        qid = qn['question_id']\n",
    "        spatial = spatials[image_id2ix[img_id]]\n",
    "\n",
    "        # read mask\n",
    "        img_mask = cv2.imread(os.path.join(gqa_data_path,f'masks/{split}/GQA_{qid}.png'))\n",
    "        img_mask = img_mask.sum(2)\n",
    "\n",
    "        bbox_impt = []\n",
    "        for i in range(spatial.shape[0]):\n",
    "            importance = calc_att_score(spatial[i, :4], img_mask)\n",
    "            bbox_impt.append(importance)\n",
    "\n",
    "        if VISUALIZE:\n",
    "            print(qn['question'])\n",
    "            plt.imshow(img_mask)\n",
    "            plt.show()\n",
    "            visualization(img_id, bbox_impt, spatial)\n",
    "    qid2hints[qid] = np.array(bbox_impt)\n",
    "    \n",
    "    pickle.dump(qid2hints, open(f\"../data/neg_gqa/hints/{split}_hints.pkl\", 'wb'))\n",
    "    return qid2hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c41af9-ac37-4ec6-beae-9b27b6dca03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gqa_data_path = '../data/neg_gqa/GQA/'\n",
    "split = 'train'\n",
    "get_hint_scores_from_masks(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901219f9-aa09-46e5-97ef-d7f8ffab0143",
   "metadata": {
    "tags": []
   },
   "source": [
    "## compare two methods for impt score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b41fb-4d74-4ffd-b742-3a0102b110fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "import re\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0ad95-1bc7-470b-8c5b-311f97a33809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(img_id, bbox_scores, spatials, MASK_ONLY=False):\n",
    "    # original image\n",
    "    img = cv2.imread(os.path.join(gqa_data_root,f'images/images/{img_id}.jpg'))\n",
    "    h, w, _ = img.shape\n",
    "    if not MASK_ONLY:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # bbox image\n",
    "    h, w, _ = img.shape\n",
    "    bbox_img = img.copy()\n",
    "    for obj in spatials:\n",
    "        x1, y1, x2, y2, _, _ = obj\n",
    "        cv2.rectangle(bbox_img, \n",
    "                      (int(x1*w), int(y1*h)), \n",
    "                      (int(x2*w), int(y2*h)), \n",
    "                      (255,0,0), 2)\n",
    "    if not MASK_ONLY:\n",
    "        plt.imshow(cv2.cvtColor(bbox_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show() \n",
    "    \n",
    "    mask = torch.zeros(img.shape[0], img.shape[1])\n",
    "    # get the max score for diff bbox\n",
    "    for index in range(len(bbox_scores)):\n",
    "        x1, y1, x2, y2, _, _ = spatials[index]\n",
    "        curr_score_tensor = mask[int(y1*h):int(y2*h), int(x1*w):int(x2*w)] \n",
    "        new_score_tensor = torch.ones_like(curr_score_tensor)*bbox_scores[index].item()\n",
    "        mask[int(y1*h):int(y2*h), int(x1*w):int(x2*w)] = torch.max(new_score_tensor,\\\n",
    "                                                                   mask[int(y1*h):int(y2*h), int(x1*w):int(x2*w)])\n",
    "    # get masked img\n",
    "    mask = mask.unsqueeze_(-1)\n",
    "    mask = mask.expand(img.shape)\n",
    "    \n",
    "    plt.imshow(cv2.cvtColor(mask.numpy(), cv2.COLOR_BGR2RGB), vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80955e-3801-4263-b0b8-eb3051c4e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impt_objs_id_from_qns(data_root, split):\n",
    "    print(f\"Reading {split} question file...\")\n",
    "    gqa_questions = json.load(open(os.path.join(data_root, 'questions', f'{split}_balanced_questions.json')))\n",
    "\n",
    "    print(f\"Finding impt objs...\")\n",
    "    qid2impt_objs_ids = {}\n",
    "    qid2imgid = {}\n",
    "    for qid, qns in tqdm(gqa_questions.items()):\n",
    "        obj_ids = []\n",
    "        for s in qns['semantic']:\n",
    "            obj_id = re.findall('[0-9]+', s['argument'])\n",
    "            obj_ids += obj_id\n",
    "        qid2impt_objs_ids[qid] = obj_ids\n",
    "        qid2imgid[qid] = qns['imageId']\n",
    "    return qid2impt_objs_ids, qid2imgid\n",
    "\n",
    "gqa_data_root = \"../data/neg_gqacp/GQA/\"\n",
    "split = 'val'\n",
    "qid2impt_objs_ids, qid2imgid = get_impt_objs_id_from_qns(gqa_data_root, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbe075-4cd2-420e-91cd-f797371b450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gqa_scenegraph = json.load(open(os.path.join(gqa_data_root, 'sceneGraph', f'{split}_sceneGraphs.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7585a70-3d22-4a83-a67d-a75ce68e11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gt bbox\n",
    "qid2gt_impt_bbox = {}\n",
    "for qid, ids_list in tqdm(qid2impt_objs_ids.items()):\n",
    "    if len(ids_list)==0:\n",
    "        continue\n",
    "        \n",
    "    img_id = qid2imgid[qid]\n",
    "    cur_scene = gqa_scenegraph[img_id]\n",
    "    w = cur_scene['width']\n",
    "    h = cur_scene['height']\n",
    "    img = np.zeros((h, w))\n",
    "    \n",
    "    gt_impt_bbox_list = []\n",
    "    for obj_id in ids_list:\n",
    "        if len(obj_id) <= 3: # accidentally includes non obj ids\n",
    "            continue \n",
    "        # get obj info\n",
    "        obj = cur_scene['objects'][obj_id]\n",
    "        obj_h = obj['h']\n",
    "        obj_w = obj['w']\n",
    "        obj_x = obj['x']\n",
    "        obj_y = obj['y']\n",
    "        \n",
    "        obj_h, obj_w = img[obj_y:obj_y+obj_h, obj_x:obj_x+obj_w].shape\n",
    "        gt_impt_bbox_list.append([obj_x / w, \n",
    "                                  obj_y / h,\n",
    "                                  (obj_x+obj_w) / w,\n",
    "                                  (obj_y+obj_h) / h])\n",
    "    qid2gt_impt_bbox[qid] = gt_impt_bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71965883-c788-4ce0-9c46-597d17c7f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3ce13-08df-4c82-b9a9-6375e4e35aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'dev'\n",
    "# read questions\n",
    "gqa_questions = json.load(open(f\"../data/neg_gqacp/questions/{split}_questions.json\"))['questions']\n",
    "# read spatials\n",
    "h5_path = f\"../data/neg_gqacp/{split}36.hdf5\"\n",
    "hf = h5py.File(h5_path, 'r')\n",
    "spatials = hf.get('spatial_features')\n",
    "# read img_id2idx\n",
    "image_id2ix = pickle.load(open(f\"../data/neg_gqacp/{split}36_imgid2img.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016c809-9ce4-41a6-ae1b-19343fffb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "qn = random.choice(gqa_questions)\n",
    "\n",
    "img_id = qn['image_id']\n",
    "qid = qn['question_id']\n",
    "spatial = spatials[image_id2ix[img_id]]\n",
    "print(qn['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a8cff-f2d8-46ae-a416-4f56aef4777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method #2\n",
    "gt_bbox_list = qid2gt_impt_bbox[qid]\n",
    "\n",
    "impt_scores = torch.zeros((spatial.shape[0],))\n",
    "for index, detected_bbox in enumerate(spatial[:, :4]):\n",
    "    for gt_bbox in gt_bbox_list:\n",
    "        iou = bops.box_iou(torch.tensor(detected_bbox).unsqueeze(0), \n",
    "                          torch.tensor(gt_bbox).unsqueeze(0))\n",
    "        impt_scores[index] = max(iou, impt_scores[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962fb07-a370-441b-bc17-8f0cea0102a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualization(img_id, impt_scores, spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075e74a-e137-401c-9867-5daa149c3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1\n",
    "def calc_att_score(bbox, att_map, SOFT=False):\n",
    "    if SOFT:\n",
    "        mask = att_map == 0\n",
    "        att_map = att_map + np.ones(att_map.shape)*(0.05*255) * mask\n",
    "    # bbox: x1, y1, x2, y2 (scaled localtion)\n",
    "    # att_map: \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    region_area = np.abs(x1 - x2) * np.abs(y1 - y2)\n",
    "    assert(len(att_map.shape) == 2)\n",
    "    h = att_map.shape[0]\n",
    "    w = att_map.shape[1]\n",
    "    \n",
    "    score_inside = np.sum(att_map[int(y1*h):int(y2*h), int(x1*w):int(x2*w)]) # DEBUG!\n",
    "    score_outside = np.sum(att_map) - score_inside\n",
    "    score_inside = score_inside / region_area\n",
    "    score_outside = score_outside / (1.0 - region_area)\n",
    "    importance = score_inside / (score_inside + score_outside)\n",
    "    return importance\n",
    "\n",
    "img_mask = cv2.imread(os.path.join(gqa_data_root,f'masks/{split}/GQA_{qid}.png'))\n",
    "img_mask = img_mask.mean(2)\n",
    "plt.imshow(img_mask,cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "bbox_impt = []\n",
    "for i in range(spatial.shape[0]):\n",
    "    importance = calc_att_score(spatial[i, :4], img_mask)\n",
    "    bbox_impt.append(importance)\n",
    "bbox_impt_soft = []\n",
    "for i in range(spatial.shape[0]):\n",
    "    importance = calc_att_score(spatial[i, :4], img_mask, SOFT=True)\n",
    "    bbox_impt_soft.append(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a1176-4190-4963-945d-aea1338b8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = visualization(img_id, bbox_impt, spatial, MASK_ONLY=True)\n",
    "mask = visualization(img_id, bbox_impt_soft, spatial, MASK_ONLY=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a489a-7b87-4d4b-b847-f88dd4920853",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get hints using IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22faa84b-0a0f-4607-8698-142c1f23e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "import re\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abe051-d7f2-4bdb-aad3-556e30cad8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impt_objs_id_from_qns(data_root, split):\n",
    "    print(f\"Reading {split} question file...\")\n",
    "    gqa_questions = json.load(open(os.path.join(data_root, 'questions', f'{split}_balanced_questions.json')))\n",
    "\n",
    "    print(f\"Finding impt objs...\")\n",
    "    qid2impt_objs_ids = {}\n",
    "    qid2imgid = {}\n",
    "    for qid, qns in tqdm(gqa_questions.items()):\n",
    "        obj_ids = []\n",
    "        for s in qns['semantic']:\n",
    "            obj_id = re.findall('[0-9]+', s['argument'])\n",
    "            obj_ids += obj_id\n",
    "        qid2impt_objs_ids[qid] = obj_ids\n",
    "        qid2imgid[qid] = qns['imageId']\n",
    "    return qid2impt_objs_ids, qid2imgid\n",
    "\n",
    "gqa_data_root = \"../data/neg_gqacp/GQA/\"\n",
    "split = 'val'\n",
    "qid2impt_objs_ids_val, qid2imgid_val = get_impt_objs_id_from_qns(gqa_data_root, split)\n",
    "split = 'train'\n",
    "qid2impt_objs_ids_train, qid2imgid_train = get_impt_objs_id_from_qns(gqa_data_root, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6bdc9-92f0-4d26-93c0-96166fa52e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge qid2impt_objs_ids & qid2imgid\n",
    "qid2impt_objs_ids_train.update(qid2impt_objs_ids_val)\n",
    "qid2impt_objs_ids = qid2impt_objs_ids_train\n",
    "qid2imgid_train.update(qid2imgid_val)\n",
    "qid2imgid = qid2imgid_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44568a63-3990-40fe-bd7d-caff145774d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del qid2imgid_val, qid2imgid_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094bedf-71f5-412d-b2c2-46daf826b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gqa_scenegraph_train = json.load(open(os.path.join(gqa_data_root, 'sceneGraph', f'train_sceneGraphs.json')))\n",
    "gqa_scenegraph_val = json.load(open(os.path.join(gqa_data_root, 'sceneGraph', f'val_sceneGraphs.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950dfb52-5d3f-426e-8727-359a62233b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge gqa_scenegraph\n",
    "gqa_scenegraph_train.update(gqa_scenegraph_val)\n",
    "gqa_scenegraph = gqa_scenegraph_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98de30-dd45-469e-b48f-5469f21a5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gqa_scenegraph_train, gqa_scenegraph_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1307b85-f2d7-487b-9208-2a4d629cbb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gt bbox\n",
    "qid2gt_impt_bbox = {}\n",
    "for qid, ids_list in tqdm(qid2impt_objs_ids.items()):\n",
    "    if len(ids_list)==0:\n",
    "        continue\n",
    "        \n",
    "    img_id = qid2imgid[qid]\n",
    "    cur_scene = gqa_scenegraph[img_id]\n",
    "    w = cur_scene['width']\n",
    "    h = cur_scene['height']\n",
    "    img = np.zeros((h, w))\n",
    "    \n",
    "    gt_impt_bbox_list = []\n",
    "    for obj_id in ids_list:\n",
    "        if len(obj_id) <= 3: # accidentally includes non obj ids\n",
    "            continue \n",
    "        # get obj info\n",
    "        obj = cur_scene['objects'][obj_id]\n",
    "        obj_h = obj['h']\n",
    "        obj_w = obj['w']\n",
    "        obj_x = obj['x']\n",
    "        obj_y = obj['y']\n",
    "        \n",
    "        obj_h, obj_w = img[obj_y:obj_y+obj_h, obj_x:obj_x+obj_w].shape\n",
    "        gt_impt_bbox_list.append([obj_x / w, \n",
    "                                  obj_y / h,\n",
    "                                  (obj_x+obj_w) / w,\n",
    "                                  (obj_y+obj_h) / h])\n",
    "    qid2gt_impt_bbox[qid] = gt_impt_bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6cef6-84ab-4620-a39c-fdbb56cfaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qn in gqa_questions_train:\n",
    "    if qn['question_id'] == '08902400':\n",
    "        print(qn)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbf57b-6b64-477b-b695-a2c7f0b8089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'08902400' in qid2gt_impt_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e06d3-1bfa-4a49-8874-6a0a35635776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read questions\n",
    "gqa_questions_train = json.load(open(f\"../data/neg_gqacp/questions/train_questions.json\"))['questions']\n",
    "gqa_questions_dev = json.load(open(f\"../data/neg_gqacp/questions/dev_questions.json\"))['questions']\n",
    "gqa_questions_test_id = json.load(open(f\"../data/neg_gqacp/questions/test-id_questions.json\"))['questions']\n",
    "gqa_questions_test_ood = json.load(open(f\"../data/neg_gqacp/questions/test-ood_questions.json\"))['questions']\n",
    "\n",
    "# read spatials\n",
    "h5_path = f\"../data/neg_gqacp/train36.hdf5\"\n",
    "hf1 = h5py.File(h5_path, 'r')\n",
    "spatials_train = hf1.get('spatial_features')\n",
    "\n",
    "h5_path = f\"../data/neg_gqacp/dev36.hdf5\"\n",
    "hf2 = h5py.File(h5_path, 'r')\n",
    "spatials_dev = hf2.get('spatial_features')\n",
    "\n",
    "h5_path = f\"../data/neg_gqacp/test-id36.hdf5\"\n",
    "hf3 = h5py.File(h5_path, 'r')\n",
    "spatials_test_id = hf3.get('spatial_features')\n",
    "\n",
    "h5_path = f\"../data/neg_gqacp/test-ood36.hdf5\"\n",
    "hf4 = h5py.File(h5_path, 'r')\n",
    "spatials_test_ood = hf4.get('spatial_features')\n",
    "# read img_id2idx\n",
    "image_id2ix_train = pickle.load(open(f\"../data/neg_gqacp/train36_imgid2img.pkl\", 'rb'))\n",
    "image_id2ix_dev = pickle.load(open(f\"../data/neg_gqacp/dev36_imgid2img.pkl\", 'rb'))\n",
    "image_id2ix_test_id = pickle.load(open(f\"../data/neg_gqacp/test-id36_imgid2img.pkl\", 'rb'))\n",
    "image_id2ix_test_ood = pickle.load(open(f\"../data/neg_gqacp/test-ood36_imgid2img.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf01a5-0b3e-45ea-af7b-f1ac7e7409cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f678f-0164-4642-a0ad-48f8578cd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou_score(gqa_questions, gqa_spatials, gqd_image_id2ix):\n",
    "    qid2iou_score = {}\n",
    "    for qn in tqdm(gqa_questions):\n",
    "        img_id = qn['image_id']\n",
    "        qid = qn['question_id']\n",
    "        spatial = gqa_spatials[gqd_image_id2ix[img_id]]\n",
    "\n",
    "        # method #2\n",
    "        if qid not in qid2gt_impt_bbox: # if no gt bbox, ignore\n",
    "            continue\n",
    "        gt_bbox_list = qid2gt_impt_bbox[qid]\n",
    "\n",
    "        impt_scores = torch.zeros((spatial.shape[0],))\n",
    "        for index, detected_bbox in enumerate(spatial[:, :4]):\n",
    "            for gt_bbox in gt_bbox_list:\n",
    "                iou = box_iou(torch.tensor(detected_bbox).unsqueeze(0), \n",
    "                                  torch.tensor(gt_bbox).unsqueeze(0))\n",
    "                impt_scores[index] = max(iou, impt_scores[index])\n",
    "        qid2iou_score[qid] = impt_scores\n",
    "    return qid2iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b41b0-9da8-4268-9fa3-644e28e5c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "hint_train = get_iou_score(gqa_questions_train, spatials_train, image_id2ix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527ac0e-1fc7-4bdb-a13f-a6dcd67859f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_dev = get_iou_score(gqa_questions_dev, spatials_dev, image_id2ix_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e16768-1146-44e0-a2a8-a81b77e83611",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_test_id = get_iou_score(gqa_questions_test_id, spatials_test_id, image_id2ix_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0d9c4-cf1e-4338-a3c4-47e099bf46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_test_ood = get_iou_score(gqa_questions_test_ood, spatials_test_ood, image_id2ix_test_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81c0a5-583b-41d1-869b-805feef117e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hint_train), len(hint_dev), len(hint_test_id), len(hint_test_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf516ab-2533-4d26-8ea8-aea85ea2c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/hints/train_hints.pkl'\n",
    "with open(_path, 'wb') as handle:\n",
    "    pickle.dump(hint_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0dfa8-fb8b-4492-89fd-52c87021861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/hints/dev_hints.pkl'\n",
    "with open(_path, 'wb') as handle:\n",
    "    pickle.dump(hint_dev, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a339d5-ab9a-4c8f-87de-c781504e6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/hints/test-id_hints.pkl'\n",
    "with open(_path, 'wb') as handle:\n",
    "    pickle.dump(hint_test_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7db16b-ca28-41e7-881d-4262e631351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/hints/test-ood_hints.pkl'\n",
    "with open(_path, 'wb') as handle:\n",
    "    pickle.dump(hint_test_ood, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa33a2-72d8-4036-8985-64fc1bcdaba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_train.update(hint_dev)\n",
    "hint_train.update(hint_test_id)\n",
    "hint_train.update(hint_test_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a7059-14f4-4f9d-a592-8457e795fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/hints/gqacp_hints.pkl'\n",
    "with open(_path, 'wb') as handle:\n",
    "    pickle.dump(hint_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd6fd8-1d38-4f73-a28e-10a224fc55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hints_random = {}\n",
    "for qid in hint_train:\n",
    "    h = np.random.rand(36)\n",
    "    hints_random[qid] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa957ee2-216a-4c49-88f7-1487f4ea06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../data/neg_gqacp/hints/gqacp_hints_random.pkl'\n",
    "with open(_path, 'wb') as handle:\n",
    "    pickle.dump(hints_random, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
